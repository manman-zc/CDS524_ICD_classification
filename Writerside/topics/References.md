# 8. References

1. [1] World Health Organization. (2004). International Statistical Classification of Diseases and related health problems: Alphabetical index (Vol. 3). World Health Organization.  
2. [2] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019, June). Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers) (pp. 4171-4186).
 
[3] Li, X., et al. (2020). Fine-tuning Pre-trained Language Models for ICD Coding: An Empirical Study. Journal of Medical Systems, 44(3).  
[4] Wang, Y., et al. (2020). Comparison of Deep Learning Methods for Automatic ICD Coding from Electronic Medical Records. IEEE Access, 8.


[5] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. *Advances in neural information processing systems*, *30*.

[6]  PaddleNLP: 飞桨自然语言处理工具集. https://github.com/PaddlePaddle/PaddleNLP

[7] Joachims, T. (1998). Text categorization with support vector machines: Learning with many relevant features. In European conference on machine learning (pp. 137-142). Springer, Berlin, Heidelberg.

[8] Kendall, A., Gal, Y., & Cipolla, R. (2018). Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7482-7491).

[9] Prechelt, L. (1998). Early stopping-but when?. In Neural Networks: Tricks of the trade (pp. 55-69). Springer, Berlin, Heidelberg.
